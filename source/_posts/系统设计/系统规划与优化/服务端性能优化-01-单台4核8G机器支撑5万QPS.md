---
title: 服务端性能优化-01-单台4核8G机器支撑5万QPS
date: 2021-05-12 00:19:51
tags:
---



# [需求描述](https://mp.weixin.qq.com/s?__biz=MzUzMTA2NTU2Ng==&mid=2247487551&idx=1&sn=18f64ba49f3f0f9d8be9d1fdef8857d9&scene=21#wechat_redirect)

这个项目是我在上家公司负责一个单独的模块，本来是集成在主站代码中的，后来因为并发太大，为了防止出现问题后拖累主站服务，所有由我一个人负责拆分出来。对这个模块的拆分要求是，压力测试QPS不能低于3万，数据库负载不能超过50%，服务器负载不能超过70%, 单次请求时长不能超过70ms，错误率不能超过5%。

环境的配置如下: 服务器：4核8G内存，centos7系统，ssd硬盘 数据库：Mysql5.7，最大连接数800 缓存: redis, 1G容量。以上环境都是购买自腾讯云的服务。压测工具：locust，使用腾讯的弹性伸缩实现分布式的压测。

需求描述如下：用户进入首页，从数据库中查询是否有合适的弹窗配置，如果没有，则继续等待下一次请求、如果有合适的配置，则返回给前端。这里开始则有多个条件分支，如果用户点击了弹窗，则记录用户点击，并且在配置的时间内不再返回配置，如果用户未点击，则24小时后继续返回本次配置，如果用户点击了，但是后续没有配置了，则接着等待下一次。

# [重点分析](https://mp.weixin.qq.com/s?__biz=MzUzMTA2NTU2Ng==&mid=2247487551&idx=1&sn=18f64ba49f3f0f9d8be9d1fdef8857d9&scene=21#wechat_redirect)

根据需求，我们知道了有几个重要的点，1、需要找出合适用户的弹窗配置，2、需要记录用户下一次返回配置的时间并记录到数据库中，3、需要记录用户对返回的配置执行了什么操作并记录到数据库中。

# [调优](https://mp.weixin.qq.com/s?__biz=MzUzMTA2NTU2Ng==&mid=2247487551&idx=1&sn=18f64ba49f3f0f9d8be9d1fdef8857d9&scene=21#wechat_redirect)

我们可以看到，上述三个重点都存在数据库的操作，不只有读库，还有写库操作。如果不加缓存的话，所有的请求都压到数据库，占满全部连接数，出现拒绝访问的错误；同时因为sql执行过慢，导致请求无法及时返回。

## 使用缓存

所以，首先要做的把[写库操作]()剥离开来，提升每一次请求响应速度，优化数据库连接。

将写库操作放到一个先进先出的消息队列中来做，为了减少复杂度，使用了redis的list来做这个消息队列。

整个系统的架构图如下：

<img src="https://mmbiz.qpic.cn/mmbiz_jpg/JdLkEI9sZfe3ib1nabj8VRu8ibxmhIwCuCPsBcrqJ0WsesHwmNVt8HRujLun8Z6y5QvQib9jCFS7QsFUQiclhtsPNA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt="Image" style="zoom:50%;" />





然后进行压测，结果如下：

QPS在6000左右502错误大幅上升至30%，服务器cpu在60%-70%之间来回跳动，数据库连接数被占满tcp连接数为6000左右，很明显，问题还是出在数据库，经过排查sql语句，查询到原因就是找出合适用户的配置操作时每次请求都要读取数据库所导致的连接数被用完。因为我们的连接数只有800，一旦请求过多，势必会导致数据库瓶颈。好了，问题找到了，我们继续优化，

## 加载缓存

更新的架构如下

<img src="https://mmbiz.qpic.cn/mmbiz_jpg/JdLkEI9sZfe3ib1nabj8VRu8ibxmhIwCuC3KkHYLc6g0ETYl1ULtZ31p8oowODKy4EMQ7CxaGQgYXuxFO57RfAPQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt="Image" style="zoom:50%;" />

我们将全部的配置都加载到缓存中，只有在缓存中没有配置的时候才会去读取数据库。

接下来我们再次压测，结果如下：QPS压到2万左右的时候就上不去了，服务器cpu在60%-80%之间跳动，数据库连接数为300个左右，每秒tpc连接数为1.5万左右。

因为2万的QPS，但是tcp连接数却并没有达到2万，猜测tcp连接数就是引发瓶颈的问题:

- 猜测1：既然是无法建立tcp连接，是否有可能是服务器限制了socket连接数，验证猜测，我们看一下，在终端输入ulimit -n命令，显示的结果为65535，看到这里，觉得socket连接数并不是限制我们的原因

- 猜测2：虽然socket连接数足够，但是并没有全部被用上，猜测，每次请求过后，tcp连接并没有立即被释放，导致socket无法重用。经过查找资料，找到了问题所在，

```
tcp链接在经过四次握手结束连接后并不会立即释放，而是处于timewait状态，会等待一段时间，以防止客户端后续的数据未被接收。
```

## 调整网络链接

首先调整tcp链接结束后等待时间，但是linux并没有提供这一内核参数的调整，如果要改，必须要自己重新编译内核，幸好还有另一个参数net.ipv4.tcp_max_tw_buckets， timewait 的数量，默认是 180000。我们调整为6000，然后打开timewait快速回收，和开启重用，完整的参数优化如下

```
#timewait 的数量，默认是 180000。
net.ipv4.tcp_max_tw_buckets = 6000

net.ipv4.ip_local_port_range = 1024 65000

#启用 timewait 快速回收。
net.ipv4.tcp_tw_recycle = 1

#开启重用。允许将 TIME-WAIT sockets 重新用于新的 TCP 连接。
net.ipv4.tcp_tw_reuse = 1
```



