---
title: redis集群-01-Redis为什么不需要Leader
date: 2021-08-02 13:57:58
tags:
---

# [为什么同样是分布式架构的Kafka需要Leader而Redis不需要](https://mp.weixin.qq.com/s/u6w0jVqHqQlWborfZbw8Uw)

Redis不需要Leader这个观点其实有歧义，是不准确的，这个问题本质其实是涉及数据分片、数据副本一致性

## Redis Cluster 架构

在Redis3.0版本开始，Redis引入了一种去中心化的集群架构，采用预分片的模式，一个集群中所有节点总共对应16384个槽位(2^32)，在对一个key进行写入时，首先对key取hashcode，然后求模来映射到具体的某一个节点，其部署架构如下图所示：

![Image](/Users/qifei/Documents/blog/source/_posts/database/redis/redis3.0-预分片模式.png)

上述每一个节点中存储的数据都不一样，即每一个节点存储整体数据的一部分，**并且为了实现去中心化**每一个节点需要存储集群中所有key所对应存档的节点信息(即Key的路由信息)，这样当客户端将查询key1的请求发送到redisA节点，但该key1实际存储在redisB节点，此时A节点需将该节点路由到实际存储该key的节点，内部实现一个重定向，从而实现访问任意一个节点都能查询到存储的值。

在**上述架构中是不需要存在Leader的，这也是所谓的集群去中心化设计思想的关键**，但问题来了，如果集群中任意一个节点宕机不可用，存储在该节点中的数据就会丢失，为了解决这个问题，通常会引入主从架构，架构图如下所示：

![Image](/Users/qifei/Documents/blog/source/_posts/database/redis/redis预分片的主从模式.png)

具体的做法是为每一个主节点引入一个或多个从节点，用来拷贝主节点的数据，上**图中的每一个虚线框表示一个复制组，也称之为副本，副本之间的数据期望完全一致。**

在主从架构中如何保证数据一致性呢？通常主从集群与客户端之间的交互方式有如下几种：

- 客户端发送写请求到Master，在Master节点写入成功就返回给客户端，同时从节点异步复制数据，**主从存在延迟，并且当主节点宕机存在丢数据的风险**。
- [同步双写]()：客户端发送写请求到Master，Master节点写入成功后，需要等待从从节点同样写入成功后才会向客户端返回成功，该方式会**增大延迟，增加主从数据延迟**，但还是**无法避免主从数据不一致**。

上述两种情况，**都无法确保数据在主从两个节点上的一致性**。

为什么[同步双写]()也无法保证数据的一致性呢？

![Image](https://mmbiz.qpic.cn/mmbiz_png/Wkp2azia4QFsTOohXaWk3gSlDiaD0rkuiccgucT7nj8ENpdBvju1Uw9enQ5SiavazGvuldEoVt3dcj9cFvC5N1icHTA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

客户端只有在master,slave同步写入成功后才会收到响应，乍一看，能提供一致性，其实不然

1. 例如将key1的数据先写入到Master节点
2. 在写入从节点的过程中出现错误，客户端会收到写入失败
3. 但此时去master中查询key1的数据，却能查询出上一次请求失败的数据，即客户端虽然收到了写入失败，但主节点却写入成功，造成了**数据语义上的不一致性**。

<aside class="notice">
即主从同步这种架构，主从节点、客户端的确认机制存在天然的不足，为了解决该问题，Raft等分布式副本数据强一致性协议就闪亮登场了。
</aside>



## 副本之间强一致性协议

为了解决数据的高可用性，避免单点故障，通常会将数据同步为多份，高可用性是解决了，但带来了另外一个问题，多个副本数据之间如何保证一致性，为了解决该问题出现了诸如 raft、paxos等一致性协议。

Raft协议的数据复制说明图如下：

![Image](/Users/qifei/Documents/blog/source/_posts/database/redis/Raft协议的数据复制.png)

图中客户端向Raft协议集群发起一个写请求，集群中的 **Leader 节点**来处理写请求

1. 首先数据先存入 Leader 节点，然后需要广播给它的所有从节点
2. 从节点接收到 Leader 节点的数据推送对数据进行存储，然后向主节点汇报存储的结果
3. **Leader 节点会对该日志的存储结果进行仲裁，如果超过集群数量的一半都成功存储了该数据**，主节点则向客户端返回写入成功，否则向客户端写入写入失败。

**如果只有主节点写入成功，但其他从节点没有写入成功，就算数据被写入到Leader节点，但这部分数据对客户端来说是不可见的。**

Raft协议主要分为两个部分：**Leader节点选举与日志复制**。

- Leader节点选举：从集群中选举一个Leader节点用于**处理数据的读写**，从节点只负责从Leader节点同步数据，并且Leader节点宕机，会自动触发选举，选举出一个新的Leader节点。

- 日志复制：数据写入主节点后，主节点需要将数据转发给从节点，只有集群中**超过半数节点**都成功将一条数据写入才向客户端返回成功。

本文只从**设计层面剖析为什么Raft协议能实现数据的一致性**。

**笔者认为Raft协议能确保数据的一致性，主要是引入了全局日志序号与已提交指针。** <!--还有一点，我认为是因为Follower不再响应客户端，而是Leader响应客户端-->

###  引入了全局日志序号

为了方便对日志进行管理与辨别，raft 协议为一条一条的消息进行编号，每一条消息达到主节点时会生成一个全局唯一的递增号，这样可以根据日志序号来快速的判断数据在主从复制过程中数据是否一致。

###  已提交指针

我们知道，日志先写入主节点，然后再进行传播，在集群中超过半数节点的写入成功之前，这条日志都不能认为**写入成功**，尽管已经存储到了主节点中，为了让客户端对这条日志不可见，Raft协议引入了**已提交指针，只有小于等于已提交的数据才能被客户端感知**。

一条日志要能被提交的充分必要条件是日志得到了集群内超过半数节点成功追加，才能被认为已提交，才会向客户端返回成功，**这样就实现了数据在集群内、客户端与集群之间的数据一致性语义**。

为了让大家更加深入的理解Raft协议数据性一致性问题，给出如下**思考题**，主从切换会导致Raft丢失数据吗？

例如一个Raft协议中有3个节点，各个节点的写入情况如下：

Node1：100

Node2：89

Node3：88

其中Node1为Leader节点，如果Node1节点宕机，整个集群触发重新选举，会丢失数据吗？

**答案是肯定不会的**。

首先我们要先明白，在上面的状态下，已提交指针为89，因为集群有两个节点都成功写入了89，即向客户端返回成功的数据也是序号为89的数据，在选举过程中，Node3不可能会被选举为Leader，因为Node3中存储的数据小于Node2存储的数据，当Node2选举为新的Leader时，Node3会向Node2同步数据。

## 3、总结

------

挖掘该问题的本质：分布式数据存储的数据分片与高可用(避免单点故障)，从而又引发新的问题（数据副本之间的一致性）

```
我稍微补充一下作者。1. 日志的唯一性是由 leader term + index 保证的。2. 选举时参与投票的节点会判断候选人的日志情况，不会投票给日志比自己少的节点，由此保证新 leader 一定有所有 commit 的日志。3. 实际主从复制也可以保证强一致性，在发生错误时不返回即可。但是这么做的问题是，引入了从节点，系统的可用性反而降低，因为一个节点挂了系统就不工作了。所以才有了多节点的一致性算法。


威总好，上面你对redis主从数据一致性无法保证举得例子不是很清晰，首先分布式节点要保证数据一致性，一般都是同步过半多阶段提交  +  过半选举配合实现的，说一致性应该需要从系统和用户两个角度分析，总结一句话就是：系统一致，不一定用户一致，系统不一致，用户肯定不一致。
 -- 嗯，所以后面引入了raft协议，举redis主从同步只是展示主从同步的缺陷
 
100不就丢了吗
 -- 100只有leader写入成功，其他节点没有成功，此时不满足超过集群半数节点，故不会向客户端返回成功

“Raft协议引入了已提交指针，只有小于等于已提交的数据才能被客户端感知”这句话不是很理解，，望解答
 --就是数据要大部分节点都写入成功，才会告知客户端成功，但数据是先写入到leader，例如文末的例子中，leader节点写入了100条，但序号为100的数据是无法被客户端查询的，因为还未提交

```

